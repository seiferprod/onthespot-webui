import linecache
import logging
import os
import sys
import tracemalloc
from functools import wraps
from logging.handlers import RotatingFileHandler
from threading import Lock
from .otsconfig import config

log_formatter = logging.Formatter(
    '[%(asctime)s :: %(name)s :: %(pathname)s -> %(lineno)s:%(funcName)20s() :: %(levelname)s] -> %(message)s'
)
log_handler = RotatingFileHandler(config.get("_log_file"),
                                  mode='a',
                                  maxBytes=(5 * 1024 * 1024),
                                  backupCount=2,
                                  encoding='utf-8',
                                  delay=0)
stdout_handler = logging.StreamHandler(sys.stdout)
log_handler.setFormatter(log_formatter)
stdout_handler.setFormatter(log_formatter)

account_pool = []
temp_download_path = []
parsing = {}
pending = {}
download_queue = {}
parsing_lock = Lock()
pending_lock = Lock()
download_queue_lock = Lock()

# System notifications for web UI
system_notifications = []
system_notifications_lock = Lock()

# Album download locks to prevent concurrent downloads from same album
album_download_locks = {}
album_download_locks_lock = Lock()

# Batch parsing state (for playlists/albums that add multiple items)
batch_parse_in_progress = False
batch_parse_lock = Lock()
batch_parse_start_time = None  # Track when flag was set

# Batch queue processing state (when QueueWorker is adding many items to download queue)
batch_queue_processing = False
batch_queue_processing_lock = Lock()
batch_queue_processing_start_time = None  # Track when flag was set

# Timeout for batch operations (in seconds)
BATCH_OPERATION_TIMEOUT = 60  # 1 minute

# Worker management
worker_threads = []
worker_threads_lock = Lock()
worker_restart_callback = None  # Function to call to restart workers (soft restart)
watchdog_restart_callback = None  # Function to call for hard restart when stuck
worker_restart_lock = Lock()  # Prevent multiple simultaneous restarts
worker_restart_in_progress = False
account_consecutive_failures = {}  # Track failures per account index
consecutive_failures_lock = Lock()
MAX_FAILURE_TRACKING_SIZE = 100  # Prevent unbounded growth

init_tray = False


def set_init_tray(value):
    global init_tray
    init_tray = value


def get_init_tray():
    return init_tray


loglevel = int(os.environ.get("LOG_LEVEL", 20))


def get_logger(name):
    logger = logging.getLogger(name)
    logger.addHandler(log_handler)
    logger.addHandler(stdout_handler)
    logger.setLevel(loglevel)
    return logger


logger_ = get_logger("runtimedata")


def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logger_.critical("Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))


sys.excepthook = handle_exception

def log_function_memory(wrap_func):
    tracemalloc.start()
    top_limit = 10
    def display_top(snapshot, snapshot_log_prefix, key_type='lineno'):
        snapshot = snapshot.filter_traces((
            tracemalloc.Filter(False, "<frozen importlib._bootstrap>"),
            tracemalloc.Filter(False, "<unknown>"),
        ))
        top_stats = snapshot.statistics(key_type)

        logger_.debug(f"{snapshot_log_prefix} Top {top_limit} lines")
        for index, stat in enumerate(top_stats[:top_limit], 1):
            frame = stat.traceback[0]
            logger_.debug("#%s: %s:%s: %.1f KiB"
                % (index, frame.filename, frame.lineno, stat.size / 1024))
            line = linecache.getline(frame.filename, frame.lineno).strip()
            if line:
                logger_.debug(f"{snapshot_log_prefix} -- {line}"  )

        other = top_stats[top_limit:]
        if other:
            size = sum(stat.size for stat in other)
            logger_.debug("%s other: %.1f KiB" % (len(other), size / 1024))
        total = sum(stat.size for stat in top_stats)
        logger_.debug("Total allocated size: %.1f KiB" % (total / 1024))

    @wraps(wrap_func)
    def snapshot_function_call(*args, **kwargs):
        prefix = f"{wrap_func.__name__}: "
        before_func = tracemalloc.take_snapshot()
        logger_.debug(f"Snapshotting before {wrap_func.__name__} call")
        ret_val = wrap_func(*args, **kwargs)
        display_top(before_func, prefix)
        logger_.debug(f"Snapshotting after {wrap_func.__name__} call")
        after_func = tracemalloc.take_snapshot()
        display_top(after_func, prefix)
        top_stats = after_func.compare_to(before_func, 'lineno')
        logger_.debug(f"{prefix} Top {top_limit} differences")
        for stat in top_stats[:10]:
            logger_.debug(f"{prefix}{stat}")
        return ret_val
    return snapshot_function_call


def register_worker(worker):
    """Register a worker thread for management"""
    with worker_threads_lock:
        worker_threads.append(worker)
        logger_.debug(f"Registered worker: {worker.__class__.__name__}, total workers: {len(worker_threads)}")


def kill_all_workers():
    """Kill all registered worker threads"""
    global worker_threads
    logger_.warning("Killing all worker threads...")
    
    # Log queue status before killing workers
    with pending_lock:
        pending_count = len(pending)
    logger_.info(f"Pending queue has {pending_count} items before worker restart")

    with worker_threads_lock:
        import threading
        current_thread = threading.current_thread()
        
        for worker in worker_threads:
            try:
                # Skip if trying to stop current thread (causes deadlock)
                if worker.thread == current_thread if hasattr(worker, 'thread') else False:
                    logger_.warning(f"Skipping stop of current thread: {worker.__class__.__name__}")
                    continue
                    
                logger_.info(f"Stopping worker: {worker.__class__.__name__}")
                worker.stop()
            except Exception as e:
                logger_.error(f"Error stopping worker {worker.__class__.__name__}: {e}")

        # Clear the list
        worker_threads = []
        logger_.info("All workers stopped and cleared")
        
    # Log queue status after restart for verification
    with pending_lock:
        pending_count_after = len(pending)
    logger_.info(f"Pending queue has {pending_count_after} items after worker restart - these will be processed by new workers")


def increment_failure_count(account_index=None):
    """Increment consecutive failure counter for an account and trigger restart if threshold reached"""
    global account_consecutive_failures, worker_restart_callback

    # Threshold for triggering worker restart (per account)
    FAILURE_THRESHOLD = 3

    with consecutive_failures_lock:
        if account_index is None:
            # If no account specified, increment a global counter
            account_index = -1
        
        # Prevent unbounded growth - cleanup if dict gets too large
        if len(account_consecutive_failures) > MAX_FAILURE_TRACKING_SIZE:
            logger_.warning(f"Failure tracking dict size ({len(account_consecutive_failures)}) exceeded limit, clearing old entries")
            # Keep only entries with high failure counts
            account_consecutive_failures = {k: v for k, v in account_consecutive_failures.items() if v >= 2}
        
        if account_index not in account_consecutive_failures:
            account_consecutive_failures[account_index] = 0
        
        account_consecutive_failures[account_index] += 1
        current_count = account_consecutive_failures[account_index]
        
        # Get account info for logging
        account_info = f"account {account_index}" if account_index >= 0 else "unknown account"
        if account_index >= 0 and account_index < len(account_pool):
            account_uuid = account_pool[account_index].get('uuid', 'unknown')
            account_info = f"account {account_index} ({account_uuid})"

    if current_count >= FAILURE_THRESHOLD:
        global worker_restart_in_progress
        
        # Check if restart already in progress
        with worker_restart_lock:
            if worker_restart_in_progress:
                logger_.warning(f"Restart already in progress, skipping duplicate restart trigger for {account_info}")
                return
            worker_restart_in_progress = True
        
        try:
            logger_.error(f"üö® CRITICAL: Consecutive failures for {account_info} reached {current_count}, triggering HARD RESTART...")

            # Reset all counters before restart to avoid repeated restarts
            reset_failure_count()

            # Trigger hard restart if callback is set
            if watchdog_restart_callback:
                try:
                    logger_.error("Executing hard restart callback now...")
                    watchdog_restart_callback()
                except Exception as e:
                    logger_.error(f"Failed to restart workers: {e}\nTraceback: {traceback.format_exc()}")
            else:
                logger_.error("No watchdog restart callback registered!")
        finally:
            # Clear restart flag after some time
            import time
            time.sleep(5)  # Give restart time to complete
            with worker_restart_lock:
                worker_restart_in_progress = False
    else:
        account_info = f"account {account_index}" if account_index >= 0 else "unknown account"
        if account_index >= 0 and account_index < len(account_pool):
            account_uuid = account_pool[account_index].get('uuid', 'unknown')
            account_info = f"account {account_index} ({account_uuid})"
        logger_.warning(f"Download failure for {account_info} ({current_count}/{FAILURE_THRESHOLD})")


def get_consecutive_failures(account_index=None):
    """Get current consecutive failure count for an account"""
    with consecutive_failures_lock:
        if account_index is None:
            # Return the maximum failure count across all accounts
            return max(account_consecutive_failures.values()) if account_consecutive_failures else 0
        return account_consecutive_failures.get(account_index, 0)


def reset_failure_count(account_index=None):
    """Reset consecutive failure counter (called on successful download)"""
    global account_consecutive_failures

    with consecutive_failures_lock:
        if account_index is None:
            # Reset all accounts
            if account_consecutive_failures:
                logger_.debug(f"Resetting all failure counts: {account_consecutive_failures}")
            account_consecutive_failures.clear()
        else:
            # Reset specific account
            if account_index in account_consecutive_failures and account_consecutive_failures[account_index] > 0:
                logger_.debug(f"Resetting failure count for account {account_index} from {account_consecutive_failures[account_index]} to 0")
            account_consecutive_failures[account_index] = 0


def set_worker_restart_callback(callback):
    """Set the callback function to restart workers (soft restart)"""
    global worker_restart_callback
    worker_restart_callback = callback
    logger_.info(f"Worker restart callback registered: {callback.__name__}")


def set_watchdog_restart_callback(callback):
    """Set the callback function for watchdog hard restart"""
    global watchdog_restart_callback
    watchdog_restart_callback = callback
    logger_.info(f"Watchdog restart callback registered: {callback.__name__}")


def trigger_worker_restart():
    """Manually trigger a worker restart (called by watchdog when download is stuck)"""
    global watchdog_restart_callback, worker_restart_in_progress
    
    # Check if restart already in progress
    with worker_restart_lock:
        if worker_restart_in_progress:
            logger_.warning("Restart already in progress, skipping duplicate watchdog trigger")
            return
        worker_restart_in_progress = True
    
    try:
        logger_.error("üö® WATCHDOG: Triggering hard worker restart due to stuck download...")
        
        # Reset all failure counters
        reset_failure_count()
        
        # Trigger hard restart if callback is set
        if watchdog_restart_callback:
            try:
                watchdog_restart_callback()
            except Exception as e:
                logger_.error(f"Failed to restart workers: {e}")
        else:
            logger_.error("No watchdog restart callback registered!")
    finally:
        # Clear restart flag after some time
        import time
        time.sleep(5)
        with worker_restart_lock:
            worker_restart_in_progress = False


def set_batch_parse_flag(value):
    """Set batch_parse_in_progress flag with timestamp tracking"""
    import time
    global batch_parse_in_progress, batch_parse_start_time
    with batch_parse_lock:
        batch_parse_in_progress = value
        batch_parse_start_time = time.time() if value else None


def set_batch_queue_processing_flag(value):
    """Set batch_queue_processing flag with timestamp tracking"""
    import time
    global batch_queue_processing, batch_queue_processing_start_time
    with batch_queue_processing_lock:
        batch_queue_processing = value
        batch_queue_processing_start_time = time.time() if value else None


def check_and_clear_stuck_flags():
    """Check if batch operation flags are stuck and trigger hard restart if timeout exceeded"""
    import time
    global batch_parse_in_progress, batch_parse_start_time
    global batch_queue_processing, batch_queue_processing_start_time
    global watchdog_restart_callback
    
    should_restart = False
    
    # Check batch_parse_in_progress flag
    with batch_parse_lock:
        if batch_parse_in_progress and batch_parse_start_time:
            elapsed = time.time() - batch_parse_start_time
            if elapsed > BATCH_OPERATION_TIMEOUT:
                logger_.error(f"‚ö†Ô∏è STUCK FLAG DETECTED: batch_parse_in_progress has been True for {elapsed:.1f}s (timeout: {BATCH_OPERATION_TIMEOUT}s)")
                logger_.error("System is unresponsive - triggering HARD RESTART to recover")
                batch_parse_in_progress = False
                batch_parse_start_time = None
                should_restart = True
    
    # Check batch_queue_processing flag
    with batch_queue_processing_lock:
        if batch_queue_processing and batch_queue_processing_start_time:
            elapsed = time.time() - batch_queue_processing_start_time
            if elapsed > BATCH_OPERATION_TIMEOUT:
                logger_.error(f"‚ö†Ô∏è STUCK FLAG DETECTED: batch_queue_processing has been True for {elapsed:.1f}s (timeout: {BATCH_OPERATION_TIMEOUT}s)")
                logger_.error("System is unresponsive - triggering HARD RESTART to recover")
                batch_queue_processing = False
                batch_queue_processing_start_time = None
                should_restart = True
    
    # Trigger hard restart if any flag was stuck
    if should_restart and watchdog_restart_callback:
        try:
            logger_.error("üîÑ WATCHDOG: Executing hard restart due to stuck flags...")
            watchdog_restart_callback()
        except Exception as e:
            logger_.error(f"Watchdog failed to restart: {e}")
    
    return should_restart
